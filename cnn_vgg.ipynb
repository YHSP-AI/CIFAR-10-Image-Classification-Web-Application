{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Ppf7FI7-Nd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dropout , Concatenate , SpatialDropout2D , GlobalAveragePooling2D  , Normalization, BatchNormalization , Activation   , AveragePooling2D  , MaxPooling2D, ZeroPadding2D , Dense , Add , ZeroPadding2D  , Input ,Flatten , Reshape\n",
        "from tensorflow.keras.models import Model , Sequential\n",
        "import tensorflow as tf \n",
        "import tensorflow.keras.backend as K \n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "mixed_precision.set_global_policy('float32')\n",
        "IMG_SIZE = 32\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "label_mapping = {0:'T-shirt/top',\n",
        "               1: 'Trouser',\n",
        "               2: 'Pullover',\n",
        "               3: 'Dress',\n",
        "               4: 'Coat',\n",
        "               5: 'Sandal',\n",
        "               6: 'Shirt',\n",
        "               7: 'Sneaker',\n",
        "               8: 'Bag',\n",
        "               9: 'Ankle Boot'}\n",
        "def plotlearningcurve(history):\n",
        "    plt.figure(figsize = (16,9))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(history.history['categorical_accuracy'] , label = 'Train Accuracy')\n",
        "    plt.plot(history.history['val_categorical_accuracy'] , label = 'Validation Accuracy')\n",
        "    plt.title('Learning Curve - Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.subplot(122)\n",
        "    plt.plot(history.history['loss'] , label = 'Train Accuracy')\n",
        "    plt.plot(history.history['val_loss'] , label = 'Validation Accuracy')\n",
        "    plt.title('Learning Curve - loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "\n",
        "    plt.legend( loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osFxcBhIolIk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np \n",
        "def class_analysis(model, X_val , y_val):\n",
        "    finalpredictions = model.predict(X_val)\n",
        "   \n",
        "    finalpredictions_final=np.argmax(finalpredictions, axis=1)\n",
        "    y_test_final=np.argmax(y_val, axis=1) if y_val.ndim == 2 else y_val\n",
        "    classificationresults = classification_report(y_test_final ,finalpredictions_final  , target_names = label_mapping.values() , output_dict =True)\n",
        "    print('Accuracy' , accuracy_score(y_test_final ,finalpredictions_final ))\n",
        "    return pd.DataFrame(classificationresults).T.sort_values('f1-score')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVl8n3zs8i71",
        "outputId": "c6d7deac-da8c-418a-d398-7f907e7f01f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "# x_train = x_train.reshape( (*x_train.shape , 1))\n",
        "# x_test = x_test.reshape( (*x_test.shape , 1))\n",
        "\n",
        "y_train= tf.keras.utils.to_categorical(y_train, num_classes=100)\n",
        "y_test= tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_and_val_size = len(x_train) \n",
        "\n",
        "print(train_and_val_size)\n",
        "trainsize= int(0.8*train_and_val_size)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((x_test ,y_test ))\n",
        "\n",
        "X_actual_train, X_val, y_actual_train, y_val = train_test_split(  x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((X_actual_train ,y_actual_train ))\n",
        "val = tf.data.Dataset.from_tensor_slices((X_val ,y_val ))\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) \n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "simpleaugpipeline = Sequential([\n",
        "    tf.keras.layers.RandomCrop(IMG_SIZE, IMG_SIZE),\n",
        "    tf.keras.layers.RandomFlip(mode = 'horizontal'),\n",
        "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2,0.2)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "train = train.map(preprocess_image)\n",
        "\n",
        "# train_augmented = train.map(lambda img,label: (simpleaugpipeline(img) , label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "                                \n",
        "def helper(*datasets):\n",
        "    imgs = [i[0] for i in datasets]\n",
        "    labels = [i[1] for i in datasets]\n",
        "\n",
        "    return tf.stack(imgs), tf.stack(labels)\n",
        "\n",
        "# fi\n",
        "\n",
        "    # for img,label in datasets:\n",
        "    #     print('img',img, 'label',label)\n",
        "    # return datasets\n",
        "datasets_zipped = tf.data.Dataset.zip((train,))\n",
        "datasets_noaug = datasets_zipped.map(helper).unbatch().map(preprocess_image).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDQ_pT46klcr"
      },
      "source": [
        "VGG + No Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OcqmBtIl9Ca"
      },
      "source": [
        "### VGG with simple augmentation \n",
        "1. `RandomCrop(IMG_SIZE, IMG_SIZE)` \n",
        "2. `RandomFlip(mode = 'horizontal')`\n",
        "3. `RandomTranslation(0.2, 0.2)`\n",
        "4. `RandomZoom(0.2,0.2)`\n",
        "\n",
        "\n",
        "\n",
        "> To Provide more stability to training\n",
        "- Dropout is decreased to 0.2 \n",
        "- L2 regularisation/ weight decay decrease to 0.0005 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVh5hLBZU3r4",
        "outputId": "93c5110f-d8f6-4946-e6ec-266c3362016c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 32, 32, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 16, 16, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " spatial_dropout2d_12 (Spati  (None, 16, 16, 32)       0         \n",
            " alDropout2D)                                                    \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 8, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " spatial_dropout2d_13 (Spati  (None, 8, 8, 64)         0         \n",
            " alDropout2D)                                                    \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 4, 4, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " spatial_dropout2d_14 (Spati  (None, 4, 4, 128)        0         \n",
            " alDropout2D)                                                    \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 4, 4, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 4, 4, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 2, 2, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " spatial_dropout2d_15 (Spati  (None, 2, 2, 256)        0         \n",
            " alDropout2D)                                                    \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 2, 2, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 2, 2, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 256)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,384,004\n",
            "Trainable params: 2,381,060\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def vggblock(x , n_conv, n_filters):\n",
        "    for _ in range(n_conv):\n",
        "        x = Conv2D(filters=n_filters,kernel_size=(3,3),padding=\"same\", activation=\"relu\" , kernel_regularizer = L2( 0.0005))(x)\n",
        "        x= BatchNormalization()(x)\n",
        "    return x\n",
        "def createcustomvgg():\n",
        "    inputtensor = Input(shape=(32,32,3))\n",
        "    x = vggblock(inputtensor, 2, 32)\n",
        "    x= MaxPooling2D((2,2) , strides =2 )(x)\n",
        "    x = SpatialDropout2D(0.2)(x)\n",
        "    x = vggblock(x , 2 , 64)\n",
        "    x= MaxPooling2D((2,2) , strides =2 )(x)\n",
        "    x = SpatialDropout2D(0.2)(x)\n",
        "    x = vggblock(x , 2 , 128)\n",
        "    x =  MaxPooling2D((2,2) , strides =2 )(x)\n",
        "    x = SpatialDropout2D(0.2)(x)\n",
        "    x = vggblock(x , 2 , 256)\n",
        "    x = MaxPooling2D((2,2) , strides =2 )(x)\n",
        "    x = SpatialDropout2D(0.2)(x)\n",
        "    x = vggblock(x, 2, 256)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x= Dense(100 , 'softmax')(x)\n",
        "    return Model( inputs = [inputtensor] , outputs = [x])\n",
        "\n",
        "vggreg2 = createcustomvgg()\n",
        "vggreg2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enCJqlDSntWF"
      },
      "source": [
        "## Cutmix Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1NCH9u9h2eZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "IMG_SIZE =32\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, (32, 32))\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) \n",
        "    return image, label\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def get_box(lambda_value):\n",
        "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n",
        "\n",
        "    cut_w = IMG_SIZE * cut_rat  # rw\n",
        "    cut_w = tf.cast(cut_w, tf.int32)\n",
        "\n",
        "    cut_h = IMG_SIZE * cut_rat  # rh\n",
        "    cut_h = tf.cast(cut_h, tf.int32)\n",
        "\n",
        "    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n",
        "    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n",
        "\n",
        "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n",
        "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n",
        "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n",
        "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n",
        "\n",
        "    target_h = bby2 - boundaryy1\n",
        "\n",
        "    target_h_new = tf.cond(target_h ==0 , lambda : target_h+1,  lambda :target_h)\n",
        "    print(type(target_h))\n",
        "    # if target_h == 0:\n",
        "    #     target_h += 1\n",
        "\n",
        "    target_w = bbx2 - boundaryx1\n",
        "    target_w_new = tf.cond(target_w ==0 , lambda:target_w+1, lambda:target_w)\n",
        "    print(target_w)\n",
        "\n",
        "    # if target_w == 0:\n",
        "    #     target_w += 1\n",
        "\n",
        "    return boundaryx1, boundaryy1, target_h_new, target_w_new\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def cutmix(train_ds_one, train_ds_two):\n",
        "    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n",
        "    # image1 = train_ds_one[0]\n",
        "    # label1 = train_ds_one[1]\n",
        "    # image2=train_ds_two[0]\n",
        "    # label2 =train_ds_two[1]\n",
        "\n",
        "    alpha = [1]\n",
        "    beta = [1]\n",
        "\n",
        "    # Get a sample from the Beta distribution\n",
        "    lambda_value = sample_beta_distribution(1, alpha, beta)\n",
        "\n",
        "    # Define Lambda\n",
        "    lambda_value = lambda_value[0][0]\n",
        "\n",
        "    # Get the bounding box offsets, heights and widths\n",
        "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n",
        "\n",
        "    # Get a patch from the second image (`image2`)\n",
        "    crop2 = tf.image.crop_to_bounding_box(\n",
        "        image2, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image2` patch (`crop2`) with the same offset\n",
        "    image2 = tf.image.pad_to_bounding_box(\n",
        "        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "    # Get a patch from the first image (`image1`)\n",
        "    crop1 = tf.image.crop_to_bounding_box(\n",
        "        image1, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image1` patch (`crop1`) with the same offset\n",
        "    img1 = tf.image.pad_to_bounding_box(\n",
        "        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "\n",
        "    # Modify the first image by subtracting the patch from `image1`\n",
        "    # (before applying the `image2` patch)\n",
        "    image1 = image1 - img1\n",
        "    # Add the modified `image1` and `image2`  together to get the CutMix image\n",
        "    image = image1 + image2\n",
        "\n",
        "    # Adjust Lambda in accordance to the pixel ration\n",
        "    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n",
        "    lambda_value = tf.cast(lambda_value, tf.float32)\n",
        "\n",
        "    # Combine the labels of both images\n",
        "    label = lambda_value * label1 + (1 - lambda_value) * label2\n",
        "    return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCSc99yMj7Zf",
        "outputId": "8a4666a0-d188-4a98-81a1-e43dbc88a62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "Tensor(\"sub_4:0\", shape=(), dtype=int32)\n",
            "Epoch 1/100\n",
            "313/313 [==============================] - 22s 55ms/step - loss: 4.9741 - categorical_accuracy: 0.0586 - val_loss: 4.8364 - val_categorical_accuracy: 0.0620\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 20s 63ms/step - loss: 4.7026 - categorical_accuracy: 0.1023 - val_loss: 4.1557 - val_categorical_accuracy: 0.1613\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 4.5540 - categorical_accuracy: 0.1231 - val_loss: 3.8610 - val_categorical_accuracy: 0.2131\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 4.4366 - categorical_accuracy: 0.1467 - val_loss: 3.7167 - val_categorical_accuracy: 0.2320\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 4.3326 - categorical_accuracy: 0.1647 - val_loss: 3.6354 - val_categorical_accuracy: 0.2400\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 4.2414 - categorical_accuracy: 0.1844 - val_loss: 3.4123 - val_categorical_accuracy: 0.2815\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 4.1601 - categorical_accuracy: 0.1954 - val_loss: 3.2637 - val_categorical_accuracy: 0.3055\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 4.0728 - categorical_accuracy: 0.2105 - val_loss: 3.1417 - val_categorical_accuracy: 0.3299\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.9931 - categorical_accuracy: 0.2293 - val_loss: 3.0094 - val_categorical_accuracy: 0.3479\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 18s 55ms/step - loss: 3.9293 - categorical_accuracy: 0.2397 - val_loss: 2.9382 - val_categorical_accuracy: 0.3678\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.8654 - categorical_accuracy: 0.2594 - val_loss: 2.8861 - val_categorical_accuracy: 0.3788\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.8041 - categorical_accuracy: 0.2668 - val_loss: 2.8252 - val_categorical_accuracy: 0.3836\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 3.7661 - categorical_accuracy: 0.2774 - val_loss: 2.7311 - val_categorical_accuracy: 0.4068\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.7110 - categorical_accuracy: 0.2873 - val_loss: 2.6233 - val_categorical_accuracy: 0.4284\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.6650 - categorical_accuracy: 0.2962 - val_loss: 2.6466 - val_categorical_accuracy: 0.4242\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.6277 - categorical_accuracy: 0.3045 - val_loss: 2.5609 - val_categorical_accuracy: 0.4400\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.5927 - categorical_accuracy: 0.3140 - val_loss: 2.4995 - val_categorical_accuracy: 0.4517\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.5480 - categorical_accuracy: 0.3210 - val_loss: 2.4421 - val_categorical_accuracy: 0.4605\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.5332 - categorical_accuracy: 0.3265 - val_loss: 2.4370 - val_categorical_accuracy: 0.4618\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.5034 - categorical_accuracy: 0.3350 - val_loss: 2.3988 - val_categorical_accuracy: 0.4716\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.4785 - categorical_accuracy: 0.3417 - val_loss: 2.3619 - val_categorical_accuracy: 0.4821\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.4510 - categorical_accuracy: 0.3473 - val_loss: 2.3839 - val_categorical_accuracy: 0.4763\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.4327 - categorical_accuracy: 0.3518 - val_loss: 2.3374 - val_categorical_accuracy: 0.4885\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.4021 - categorical_accuracy: 0.3581 - val_loss: 2.4479 - val_categorical_accuracy: 0.4603\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.3878 - categorical_accuracy: 0.3645 - val_loss: 2.2879 - val_categorical_accuracy: 0.4988\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.3811 - categorical_accuracy: 0.3634 - val_loss: 2.2908 - val_categorical_accuracy: 0.4984\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.3615 - categorical_accuracy: 0.3711 - val_loss: 2.2405 - val_categorical_accuracy: 0.5082\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.3353 - categorical_accuracy: 0.3775 - val_loss: 2.3423 - val_categorical_accuracy: 0.4874\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.3341 - categorical_accuracy: 0.3745 - val_loss: 2.4964 - val_categorical_accuracy: 0.4514\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.3140 - categorical_accuracy: 0.3862 - val_loss: 2.2498 - val_categorical_accuracy: 0.5045\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.2987 - categorical_accuracy: 0.3927 - val_loss: 2.2032 - val_categorical_accuracy: 0.5205\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.2973 - categorical_accuracy: 0.3938 - val_loss: 2.2101 - val_categorical_accuracy: 0.5216\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.2724 - categorical_accuracy: 0.4010 - val_loss: 2.1844 - val_categorical_accuracy: 0.5198\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.2735 - categorical_accuracy: 0.4014 - val_loss: 2.1887 - val_categorical_accuracy: 0.5193\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.2569 - categorical_accuracy: 0.4032 - val_loss: 2.1315 - val_categorical_accuracy: 0.5339\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 17s 52ms/step - loss: 3.2438 - categorical_accuracy: 0.4061 - val_loss: 2.1248 - val_categorical_accuracy: 0.5369\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.2427 - categorical_accuracy: 0.4108 - val_loss: 2.1758 - val_categorical_accuracy: 0.5302\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.2328 - categorical_accuracy: 0.4134 - val_loss: 2.1323 - val_categorical_accuracy: 0.5400\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.2288 - categorical_accuracy: 0.4162 - val_loss: 2.1720 - val_categorical_accuracy: 0.5313\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.2158 - categorical_accuracy: 0.4224 - val_loss: 2.1990 - val_categorical_accuracy: 0.5191\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.2185 - categorical_accuracy: 0.4227 - val_loss: 2.1077 - val_categorical_accuracy: 0.5478\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.2020 - categorical_accuracy: 0.4246 - val_loss: 2.1735 - val_categorical_accuracy: 0.5347\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.2103 - categorical_accuracy: 0.4251 - val_loss: 2.0655 - val_categorical_accuracy: 0.5589\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.2075 - categorical_accuracy: 0.4275 - val_loss: 2.1471 - val_categorical_accuracy: 0.5387\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.1873 - categorical_accuracy: 0.4300 - val_loss: 2.1430 - val_categorical_accuracy: 0.5439\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.1916 - categorical_accuracy: 0.4324 - val_loss: 2.1092 - val_categorical_accuracy: 0.5491\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - 17s 52ms/step - loss: 3.1808 - categorical_accuracy: 0.4337 - val_loss: 2.1644 - val_categorical_accuracy: 0.5354\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.1831 - categorical_accuracy: 0.4333 - val_loss: 2.1105 - val_categorical_accuracy: 0.5543\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 3.1781 - categorical_accuracy: 0.4390 - val_loss: 2.1430 - val_categorical_accuracy: 0.5402\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.1641 - categorical_accuracy: 0.4421 - val_loss: 2.1311 - val_categorical_accuracy: 0.5458\n",
            "Epoch 51/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.1666 - categorical_accuracy: 0.4418 - val_loss: 2.2672 - val_categorical_accuracy: 0.5174\n",
            "Epoch 52/100\n",
            "313/313 [==============================] - 17s 52ms/step - loss: 3.1593 - categorical_accuracy: 0.4477 - val_loss: 2.1105 - val_categorical_accuracy: 0.5559\n",
            "Epoch 53/100\n",
            "313/313 [==============================] - 18s 55ms/step - loss: 3.1486 - categorical_accuracy: 0.4510 - val_loss: 2.0916 - val_categorical_accuracy: 0.5627\n",
            "Epoch 54/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.1491 - categorical_accuracy: 0.4489 - val_loss: 2.1492 - val_categorical_accuracy: 0.5498\n",
            "Epoch 55/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 3.1601 - categorical_accuracy: 0.4473 - val_loss: 2.1189 - val_categorical_accuracy: 0.5565\n",
            "Epoch 56/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 3.1485 - categorical_accuracy: 0.4522 - val_loss: 2.1178 - val_categorical_accuracy: 0.5599\n",
            "Epoch 57/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.1491 - categorical_accuracy: 0.4540 - val_loss: 2.0780 - val_categorical_accuracy: 0.5655\n",
            "Epoch 58/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.1345 - categorical_accuracy: 0.4571 - val_loss: 2.0544 - val_categorical_accuracy: 0.5738\n",
            "Epoch 59/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 3.1519 - categorical_accuracy: 0.4566 - val_loss: 2.1054 - val_categorical_accuracy: 0.5619\n",
            "Epoch 60/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.1488 - categorical_accuracy: 0.4578 - val_loss: 2.0700 - val_categorical_accuracy: 0.5706\n",
            "Epoch 61/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.1458 - categorical_accuracy: 0.4560 - val_loss: 2.0721 - val_categorical_accuracy: 0.5705\n",
            "Epoch 62/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.1225 - categorical_accuracy: 0.4633 - val_loss: 2.0973 - val_categorical_accuracy: 0.5631\n",
            "Epoch 63/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.1292 - categorical_accuracy: 0.4638 - val_loss: 2.1042 - val_categorical_accuracy: 0.5660\n",
            "Epoch 64/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.1356 - categorical_accuracy: 0.4661 - val_loss: 2.0932 - val_categorical_accuracy: 0.5676\n",
            "Epoch 65/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.1340 - categorical_accuracy: 0.4651 - val_loss: 2.1856 - val_categorical_accuracy: 0.5431\n",
            "Epoch 66/100\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 3.1302 - categorical_accuracy: 0.4675 - val_loss: 2.0922 - val_categorical_accuracy: 0.5708\n",
            "Epoch 67/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 3.1297 - categorical_accuracy: 0.4674 - val_loss: 2.0704 - val_categorical_accuracy: 0.5728\n",
            "Epoch 68/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.1175 - categorical_accuracy: 0.4723 - val_loss: 2.1396 - val_categorical_accuracy: 0.5555\n"
          ]
        }
      ],
      "source": [
        "# tf.config.run_functions_eagerly(False)\n",
        "\n",
        "vggreg3 = tf.keras.models.clone_model(vggreg2)\n",
        "\n",
        "# Compile model\n",
        "vggreg3.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum = 0.9 ),\n",
        "    metrics=['categorical_accuracy'],\n",
        ")\n",
        "train_augmented = tf.data.Dataset.zip((train.shuffle(2048) , train.shuffle(2048))).map(cutmix, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "datasets_zipped = tf.data.Dataset.zip((train_augmented, ))\n",
        "datasets_noaug = datasets_zipped.map(helper).unbatch().batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "history2 = vggreg3.fit(datasets_noaug,\n",
        "          epochs=100,\n",
        "          validation_data = val.batch(128), \n",
        "          callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_categorical_accuracy' , patience = 10 , restore_best_weights = True  )]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uOGQ3AmLZNCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVfGIxrxYn53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vggreg3.save('vggmodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iAVX4vg9_AW",
        "outputId": "90cd593d-7a92-41cb-9f6d-fe2eddf9b7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "import requests\n",
        "import base64\n",
        "import json\n",
        "from tensorflow.keras.datasets.cifar100 import load_data\n",
        "import numpy as np\n",
        "#load MNIST dataset\n",
        "(_, _), (x_test, y_test) = load_data()\n",
        "# reshape data to have a single channel\n",
        "# x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2],\n",
        "# 1))\n",
        "# normalize pixel values\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "#server URL\n",
        "import tensorflow as tf\n",
        "vgg = tf.keras.models.load_model('vggmodel')\n",
        "url = 'https://ca2-tensorflow-serving-model-deployment.onrender.com/v1/models/vgg:predict' #see [B]\n",
        "def make_prediction(instances):\n",
        "    data = json.dumps({\"signature_name\": \"serving_default\",\n",
        "    \"instances\": instances.tolist()}) #see [C]\n",
        "    headers = {\"content-type\": \"application/json\"}\n",
        "    json_response = requests.post(url, data=data, headers=headers)\n",
        "    \n",
        "    print(json_response.text)\n",
        "    predictions = json.loads(json_response.text)['predictions']\n",
        "    return predictions\n",
        "def test_prediction():\n",
        "    predictions = make_prediction(x_test[0:4]) #see [A]\n",
        "    print(predictions)\n",
        "    # for i, pred in enumerate(predictions):\n",
        "    #     assert y_test[i] == np.argmax(pred) #see [D]\n",
        "test_prediction() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG5RNERsEWUP",
        "outputId": "25c50d88-6798-42a0-e44b-61bc20c05e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 229ms/step\n",
            "[[1.92493157e-04 9.58798119e-05 1.25727023e-03 8.20443325e-04\n",
            "  1.08714250e-03 2.00509792e-04 5.23703988e-04 1.58278213e-03\n",
            "  2.18193326e-03 1.15511146e-04 2.91189819e-04 1.77381048e-03\n",
            "  3.71770747e-02 5.36759151e-04 4.18511714e-04 1.74854079e-03\n",
            "  2.29002166e-04 1.50766559e-02 7.52128719e-04 5.29776909e-04\n",
            "  1.86728299e-04 9.53660696e-04 8.53074074e-04 2.23987512e-02\n",
            "  2.04341509e-03 8.00129608e-04 2.51153484e-03 1.10829214e-03\n",
            "  7.82754621e-04 1.25557173e-03 5.68836220e-02 7.53687869e-04\n",
            "  5.06190350e-04 6.98749558e-04 4.83619806e-04 9.46124201e-04\n",
            "  6.72343071e-04 9.09417402e-03 1.16725848e-03 4.34757530e-04\n",
            "  1.49452104e-03 1.64703553e-04 4.93756204e-04 7.72553263e-04\n",
            "  1.11211743e-02 4.14603070e-04 1.49394327e-03 3.51985567e-04\n",
            "  1.15482521e-03 9.90608707e-02 3.46994144e-03 1.87983765e-04\n",
            "  3.49691109e-04 1.53004366e-04 2.57235835e-04 4.04803827e-03\n",
            "  3.74925067e-03 6.06349262e-04 3.11662763e-04 3.94965429e-03\n",
            "  4.90245875e-03 4.19122633e-04 3.31819290e-04 7.79219787e-04\n",
            "  1.12722570e-03 4.01791331e-04 2.23945244e-03 8.21958762e-04\n",
            "  4.99887973e-01 1.03960335e-02 1.07052889e-04 3.63538787e-02\n",
            "  4.65488015e-03 1.59419596e-03 1.55391311e-03 2.28957491e-04\n",
            "  2.40463018e-02 3.17758857e-03 8.51879362e-03 3.32444981e-02\n",
            "  3.04743153e-04 2.90757325e-03 8.60737404e-04 2.17869281e-04\n",
            "  6.17943588e-04 1.51802599e-02 1.74196553e-04 3.50197050e-04\n",
            "  6.31497649e-04 1.00048399e-03 2.26907004e-02 4.58207040e-04\n",
            "  3.34213255e-04 6.30870753e-04 7.64450306e-05 6.30101096e-03\n",
            "  3.30979703e-04 1.30424206e-03 1.25983567e-03 4.84955078e-03]\n",
            " [8.03890725e-05 3.90335656e-04 6.49115129e-04 2.37443461e-03\n",
            "  3.21324705e-03 4.49926127e-04 2.45849922e-04 5.21260081e-05\n",
            "  5.66926785e-04 5.61882567e-04 2.35591055e-04 1.06361602e-03\n",
            "  1.76521658e-03 7.35484820e-04 1.50098218e-04 4.97161527e-04\n",
            "  3.32261348e-04 3.44139477e-03 2.92636035e-03 5.59601001e-04\n",
            "  4.87599245e-05 5.27840049e-04 1.69834180e-04 1.66376738e-03\n",
            "  5.17603294e-05 8.59386811e-04 6.74035982e-04 6.69950852e-04\n",
            "  2.58635351e-04 1.68597407e-03 1.48158637e-03 5.26700635e-03\n",
            "  2.82056862e-03 4.60126311e-01 4.27431660e-03 1.59775303e-03\n",
            "  4.55168076e-04 1.12963011e-02 3.46591212e-02 5.14626736e-04\n",
            "  3.28522408e-04 2.80337088e-04 5.46259247e-03 1.14856567e-03\n",
            "  5.41828107e-04 2.66305753e-03 4.21893288e-04 1.43968062e-02\n",
            "  9.05514098e-05 4.02696984e-04 2.38275155e-03 3.63415782e-03\n",
            "  4.46515251e-03 6.15049066e-05 2.69148103e-03 6.04951812e-04\n",
            "  6.22518808e-02 2.76955718e-04 2.91622448e-04 5.57009093e-02\n",
            "  2.80567241e-04 1.13010123e-04 8.21580543e-05 6.84740627e-03\n",
            "  1.23679172e-02 1.94188803e-02 4.54417989e-03 5.10569289e-03\n",
            "  9.43495834e-04 1.81963935e-03 2.26086311e-04 4.97420493e-04\n",
            "  1.04839972e-03 7.39321345e-04 3.30482912e-03 1.51752291e-04\n",
            "  1.19192689e-03 5.38688328e-04 1.81068375e-04 1.56367756e-03\n",
            "  4.29752953e-02 6.84259168e-04 9.83325663e-05 1.82096759e-04\n",
            "  6.22337172e-03 1.79356110e-04 1.50234118e-04 2.31038677e-04\n",
            "  3.24304611e-03 1.57235516e-03 2.75672821e-04 3.26803303e-04\n",
            "  1.63505931e-04 1.09377108e-03 2.28004501e-04 4.73700435e-04\n",
            "  9.21083242e-02 8.03501010e-02 4.57888731e-04 5.24754811e-04]\n",
            " [6.87087959e-05 1.31891895e-04 9.40132188e-04 1.19635714e-02\n",
            "  2.63321940e-02 3.34852462e-04 2.28602835e-03 1.51462678e-03\n",
            "  1.86221022e-03 2.72365549e-04 9.06752030e-05 1.56533974e-03\n",
            "  6.60531083e-03 6.45349850e-04 1.40018834e-04 7.05410214e-03\n",
            "  9.98959877e-05 2.18871702e-03 5.44464169e-03 1.84979185e-03\n",
            "  5.38057357e-04 5.48475189e-04 3.38830461e-04 1.40955485e-03\n",
            "  1.01592513e-02 4.02432110e-04 9.21202265e-03 4.15738747e-02\n",
            "  1.24748098e-04 6.58736657e-03 3.80915195e-01 5.47952158e-03\n",
            "  1.01083366e-03 2.17234087e-03 2.54643112e-02 7.35191396e-04\n",
            "  5.45962423e-04 2.75672576e-03 5.41745778e-03 2.27390416e-03\n",
            "  1.91946951e-04 1.52610955e-04 3.53497127e-03 1.28599769e-03\n",
            "  2.68909708e-02 5.03990278e-02 7.99946079e-04 7.61349802e-05\n",
            "  7.66075333e-04 3.59211378e-02 6.29820221e-04 2.86416779e-03\n",
            "  2.32939376e-04 5.13855775e-05 9.76016381e-05 7.41182715e-02\n",
            "  2.54107174e-03 2.19470021e-04 4.45180398e-04 1.52355526e-03\n",
            "  3.39702563e-03 1.42151810e-04 1.50882042e-04 1.35866646e-03\n",
            "  1.75681105e-03 2.81037227e-03 6.87205582e-04 1.22405810e-03\n",
            "  1.90333300e-03 3.04341083e-03 1.17900614e-04 2.21490138e-03\n",
            "  5.44715486e-02 6.31244993e-03 2.73369369e-03 1.15248270e-03\n",
            "  3.10784206e-03 1.68691156e-03 3.08548659e-03 2.49240976e-02\n",
            "  3.02795414e-03 2.92247685e-04 1.96292924e-04 9.15192868e-05\n",
            "  1.43461151e-03 1.98120670e-03 1.18053846e-04 7.92078790e-05\n",
            "  7.08395895e-03 1.03091064e-03 5.39053231e-03 9.83021036e-03\n",
            "  1.75379624e-04 6.08876906e-03 9.38375088e-05 6.32017553e-02\n",
            "  2.25864234e-04 9.25509166e-03 7.02277233e-04 1.62023306e-03]\n",
            " [3.32278782e-03 1.59410993e-03 3.26302531e-03 3.19821984e-02\n",
            "  1.37749063e-02 2.88682408e-03 2.91969511e-04 4.97066008e-04\n",
            "  1.32166385e-03 1.84157013e-03 5.40872291e-03 7.22063798e-03\n",
            "  6.53932663e-03 1.48288803e-02 4.46982420e-04 1.13839973e-02\n",
            "  8.72618109e-02 1.38395827e-03 1.59815676e-03 2.23141303e-03\n",
            "  1.07891124e-03 6.18854314e-02 1.63544603e-02 8.92289390e-04\n",
            "  2.07419987e-04 5.36201708e-03 1.08698802e-03 3.65911331e-03\n",
            "  2.45255511e-03 1.42804161e-03 4.22844291e-03 8.89510754e-03\n",
            "  3.18123749e-03 6.29686890e-03 4.67820745e-03 4.65408387e-03\n",
            "  2.01608101e-03 9.54082608e-03 4.17094911e-03 1.24441553e-03\n",
            "  3.06373425e-02 1.38621044e-03 2.51952885e-03 1.45125866e-03\n",
            "  2.50905147e-03 4.52060485e-04 2.64215888e-03 3.37865320e-04\n",
            "  2.14090454e-03 4.53130284e-04 3.53677291e-03 7.30448887e-02\n",
            "  3.55047930e-04 2.57488282e-04 1.10305264e-03 5.97709790e-02\n",
            "  1.96904712e-03 9.35564865e-04 2.75896080e-02 4.31189733e-03\n",
            "  6.02927234e-04 3.12008103e-03 1.63448785e-04 5.28039224e-03\n",
            "  9.37185809e-03 6.50342926e-03 4.64491770e-02 3.14772502e-03\n",
            "  3.69369984e-04 2.31881207e-03 2.98613129e-04 1.89645274e-03\n",
            "  7.26653188e-02 1.16807350e-03 4.88285441e-03 1.43338619e-02\n",
            "  1.13346882e-03 1.61327142e-02 1.36004174e-02 1.22779410e-03\n",
            "  2.70833988e-02 1.08503364e-02 1.73039589e-04 1.62773323e-03\n",
            "  1.38129750e-02 6.50595687e-03 2.11771065e-03 7.30268136e-02\n",
            "  1.70764478e-03 7.00734835e-03 4.06918302e-03 3.60022485e-02\n",
            "  3.98485136e-04 1.82475639e-03 1.27682146e-02 2.11365893e-03\n",
            "  7.22452300e-04 2.69705690e-02 2.28794478e-03 4.46539512e-03]]\n",
            "[[1.92493157e-04 9.58798119e-05 1.25727023e-03 8.20443325e-04\n",
            "  1.08714250e-03 2.00509792e-04 5.23703988e-04 1.58278213e-03\n",
            "  2.18193326e-03 1.15511146e-04 2.91189819e-04 1.77381048e-03\n",
            "  3.71770747e-02 5.36759151e-04 4.18511714e-04 1.74854079e-03\n",
            "  2.29002166e-04 1.50766559e-02 7.52128719e-04 5.29776909e-04\n",
            "  1.86728299e-04 9.53660696e-04 8.53074074e-04 2.23987512e-02\n",
            "  2.04341509e-03 8.00129608e-04 2.51153484e-03 1.10829214e-03\n",
            "  7.82754621e-04 1.25557173e-03 5.68836220e-02 7.53687869e-04\n",
            "  5.06190350e-04 6.98749558e-04 4.83619806e-04 9.46124201e-04\n",
            "  6.72343071e-04 9.09417402e-03 1.16725848e-03 4.34757530e-04\n",
            "  1.49452104e-03 1.64703553e-04 4.93756204e-04 7.72553263e-04\n",
            "  1.11211743e-02 4.14603070e-04 1.49394327e-03 3.51985567e-04\n",
            "  1.15482521e-03 9.90608707e-02 3.46994144e-03 1.87983765e-04\n",
            "  3.49691109e-04 1.53004366e-04 2.57235835e-04 4.04803827e-03\n",
            "  3.74925067e-03 6.06349262e-04 3.11662763e-04 3.94965429e-03\n",
            "  4.90245875e-03 4.19122633e-04 3.31819290e-04 7.79219787e-04\n",
            "  1.12722570e-03 4.01791331e-04 2.23945244e-03 8.21958762e-04\n",
            "  4.99887973e-01 1.03960335e-02 1.07052889e-04 3.63538787e-02\n",
            "  4.65488015e-03 1.59419596e-03 1.55391311e-03 2.28957491e-04\n",
            "  2.40463018e-02 3.17758857e-03 8.51879362e-03 3.32444981e-02\n",
            "  3.04743153e-04 2.90757325e-03 8.60737404e-04 2.17869281e-04\n",
            "  6.17943588e-04 1.51802599e-02 1.74196553e-04 3.50197050e-04\n",
            "  6.31497649e-04 1.00048399e-03 2.26907004e-02 4.58207040e-04\n",
            "  3.34213255e-04 6.30870753e-04 7.64450306e-05 6.30101096e-03\n",
            "  3.30979703e-04 1.30424206e-03 1.25983567e-03 4.84955078e-03]\n",
            " [8.03890725e-05 3.90335656e-04 6.49115129e-04 2.37443461e-03\n",
            "  3.21324705e-03 4.49926127e-04 2.45849922e-04 5.21260081e-05\n",
            "  5.66926785e-04 5.61882567e-04 2.35591055e-04 1.06361602e-03\n",
            "  1.76521658e-03 7.35484820e-04 1.50098218e-04 4.97161527e-04\n",
            "  3.32261348e-04 3.44139477e-03 2.92636035e-03 5.59601001e-04\n",
            "  4.87599245e-05 5.27840049e-04 1.69834180e-04 1.66376738e-03\n",
            "  5.17603294e-05 8.59386811e-04 6.74035982e-04 6.69950852e-04\n",
            "  2.58635351e-04 1.68597407e-03 1.48158637e-03 5.26700635e-03\n",
            "  2.82056862e-03 4.60126311e-01 4.27431660e-03 1.59775303e-03\n",
            "  4.55168076e-04 1.12963011e-02 3.46591212e-02 5.14626736e-04\n",
            "  3.28522408e-04 2.80337088e-04 5.46259247e-03 1.14856567e-03\n",
            "  5.41828107e-04 2.66305753e-03 4.21893288e-04 1.43968062e-02\n",
            "  9.05514098e-05 4.02696984e-04 2.38275155e-03 3.63415782e-03\n",
            "  4.46515251e-03 6.15049066e-05 2.69148103e-03 6.04951812e-04\n",
            "  6.22518808e-02 2.76955718e-04 2.91622448e-04 5.57009093e-02\n",
            "  2.80567241e-04 1.13010123e-04 8.21580543e-05 6.84740627e-03\n",
            "  1.23679172e-02 1.94188803e-02 4.54417989e-03 5.10569289e-03\n",
            "  9.43495834e-04 1.81963935e-03 2.26086311e-04 4.97420493e-04\n",
            "  1.04839972e-03 7.39321345e-04 3.30482912e-03 1.51752291e-04\n",
            "  1.19192689e-03 5.38688328e-04 1.81068375e-04 1.56367756e-03\n",
            "  4.29752953e-02 6.84259168e-04 9.83325663e-05 1.82096759e-04\n",
            "  6.22337172e-03 1.79356110e-04 1.50234118e-04 2.31038677e-04\n",
            "  3.24304611e-03 1.57235516e-03 2.75672821e-04 3.26803303e-04\n",
            "  1.63505931e-04 1.09377108e-03 2.28004501e-04 4.73700435e-04\n",
            "  9.21083242e-02 8.03501010e-02 4.57888731e-04 5.24754811e-04]\n",
            " [6.87087959e-05 1.31891895e-04 9.40132188e-04 1.19635714e-02\n",
            "  2.63321940e-02 3.34852462e-04 2.28602835e-03 1.51462678e-03\n",
            "  1.86221022e-03 2.72365549e-04 9.06752030e-05 1.56533974e-03\n",
            "  6.60531083e-03 6.45349850e-04 1.40018834e-04 7.05410214e-03\n",
            "  9.98959877e-05 2.18871702e-03 5.44464169e-03 1.84979185e-03\n",
            "  5.38057357e-04 5.48475189e-04 3.38830461e-04 1.40955485e-03\n",
            "  1.01592513e-02 4.02432110e-04 9.21202265e-03 4.15738747e-02\n",
            "  1.24748098e-04 6.58736657e-03 3.80915195e-01 5.47952158e-03\n",
            "  1.01083366e-03 2.17234087e-03 2.54643112e-02 7.35191396e-04\n",
            "  5.45962423e-04 2.75672576e-03 5.41745778e-03 2.27390416e-03\n",
            "  1.91946951e-04 1.52610955e-04 3.53497127e-03 1.28599769e-03\n",
            "  2.68909708e-02 5.03990278e-02 7.99946079e-04 7.61349802e-05\n",
            "  7.66075333e-04 3.59211378e-02 6.29820221e-04 2.86416779e-03\n",
            "  2.32939376e-04 5.13855775e-05 9.76016381e-05 7.41182715e-02\n",
            "  2.54107174e-03 2.19470021e-04 4.45180398e-04 1.52355526e-03\n",
            "  3.39702563e-03 1.42151810e-04 1.50882042e-04 1.35866646e-03\n",
            "  1.75681105e-03 2.81037227e-03 6.87205582e-04 1.22405810e-03\n",
            "  1.90333300e-03 3.04341083e-03 1.17900614e-04 2.21490138e-03\n",
            "  5.44715486e-02 6.31244993e-03 2.73369369e-03 1.15248270e-03\n",
            "  3.10784206e-03 1.68691156e-03 3.08548659e-03 2.49240976e-02\n",
            "  3.02795414e-03 2.92247685e-04 1.96292924e-04 9.15192868e-05\n",
            "  1.43461151e-03 1.98120670e-03 1.18053846e-04 7.92078790e-05\n",
            "  7.08395895e-03 1.03091064e-03 5.39053231e-03 9.83021036e-03\n",
            "  1.75379624e-04 6.08876906e-03 9.38375088e-05 6.32017553e-02\n",
            "  2.25864234e-04 9.25509166e-03 7.02277233e-04 1.62023306e-03]\n",
            " [3.32278782e-03 1.59410993e-03 3.26302531e-03 3.19821984e-02\n",
            "  1.37749063e-02 2.88682408e-03 2.91969511e-04 4.97066008e-04\n",
            "  1.32166385e-03 1.84157013e-03 5.40872291e-03 7.22063798e-03\n",
            "  6.53932663e-03 1.48288803e-02 4.46982420e-04 1.13839973e-02\n",
            "  8.72618109e-02 1.38395827e-03 1.59815676e-03 2.23141303e-03\n",
            "  1.07891124e-03 6.18854314e-02 1.63544603e-02 8.92289390e-04\n",
            "  2.07419987e-04 5.36201708e-03 1.08698802e-03 3.65911331e-03\n",
            "  2.45255511e-03 1.42804161e-03 4.22844291e-03 8.89510754e-03\n",
            "  3.18123749e-03 6.29686890e-03 4.67820745e-03 4.65408387e-03\n",
            "  2.01608101e-03 9.54082608e-03 4.17094911e-03 1.24441553e-03\n",
            "  3.06373425e-02 1.38621044e-03 2.51952885e-03 1.45125866e-03\n",
            "  2.50905147e-03 4.52060485e-04 2.64215888e-03 3.37865320e-04\n",
            "  2.14090454e-03 4.53130284e-04 3.53677291e-03 7.30448887e-02\n",
            "  3.55047930e-04 2.57488282e-04 1.10305264e-03 5.97709790e-02\n",
            "  1.96904712e-03 9.35564865e-04 2.75896080e-02 4.31189733e-03\n",
            "  6.02927234e-04 3.12008103e-03 1.63448785e-04 5.28039224e-03\n",
            "  9.37185809e-03 6.50342926e-03 4.64491770e-02 3.14772502e-03\n",
            "  3.69369984e-04 2.31881207e-03 2.98613129e-04 1.89645274e-03\n",
            "  7.26653188e-02 1.16807350e-03 4.88285441e-03 1.43338619e-02\n",
            "  1.13346882e-03 1.61327142e-02 1.36004174e-02 1.22779410e-03\n",
            "  2.70833988e-02 1.08503364e-02 1.73039589e-04 1.62773323e-03\n",
            "  1.38129750e-02 6.50595687e-03 2.11771065e-03 7.30268136e-02\n",
            "  1.70764478e-03 7.00734835e-03 4.06918302e-03 3.60022485e-02\n",
            "  3.98485136e-04 1.82475639e-03 1.27682146e-02 2.11365893e-03\n",
            "  7.22452300e-04 2.69705690e-02 2.28794478e-03 4.46539512e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vggreg3.save('vggmodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQyv5mW6Zjq5",
        "outputId": "0682502a-2d30-4059-8de9-d79dd29cfb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('vggmodel')"
      ],
      "metadata": {
        "id": "I9n41TnLZo4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR-OyFHbZtzh",
        "outputId": "2706d776-1342-4b8e-95e9-784a7f35abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " spatial_dropout2d (SpatialD  (None, 16, 16, 32)       0         \n",
            " ropout2D)                                                       \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " spatial_dropout2d_1 (Spatia  (None, 8, 8, 64)         0         \n",
            " lDropout2D)                                                     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " spatial_dropout2d_2 (Spatia  (None, 4, 4, 128)        0         \n",
            " lDropout2D)                                                     \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " spatial_dropout2d_3 (Spatia  (None, 2, 2, 256)        0         \n",
            " lDropout2D)                                                     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 256)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,384,004\n",
            "Trainable params: 2,381,060\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "b670674fb3331b150c278e991d93d887e8988d2fce5871f83074650817620005"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}